{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f25b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio\n",
    "# %pip install pandas scikit-learn\n",
    "# %pip install pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Scan Victor Kitov's dataset and create manifest.csv with four columns:\n",
    "path,label,content_id,style_id\n",
    "\"\"\"\n",
    "import re, csv, pathlib\n",
    "\n",
    "# ---- 1.1  Configure root directory & output ------------------------------\n",
    "ROOT = pathlib.Path(\"images\")  # <- change if needed\n",
    "OUT  = ROOT / \"manifest.csv\"\n",
    "\n",
    "# ---- 1.2  Regex helpers to grab IDs from filenames -----------------------\n",
    "CONTENT_RE = re.compile(r\"content_(\\d+)\")\n",
    "STYLE_RE   = re.compile(r\"style_(\\d+)\")\n",
    "STYL_RE    = re.compile(r\"content_(\\d+)___style_(\\d+)___\\d+\")\n",
    "\n",
    "# ---- 1.3  Collect rows ----------------------------------------------------\n",
    "rows = []\n",
    "\n",
    "# a) real CONTENT images\n",
    "for p in (ROOT / \"contents\").glob(\"*\"):\n",
    "    cid = int(CONTENT_RE.search(p.name)[1])  # extract content id\n",
    "    rows.append((str(p), 0, cid, -1))        # -1 style_id means “none”\n",
    "\n",
    "# b) real STYLE images\n",
    "for p in (ROOT / \"styles\").glob(\"*\"):\n",
    "    sid = int(STYLE_RE.search(p.name)[1])    # extract style id\n",
    "    rows.append((str(p), 0, -1, sid))        # -1 content_id means “none”\n",
    "\n",
    "# c) FAKE stylizations\n",
    "for p in (ROOT / \"stylizations\").glob(\"*\"):\n",
    "    cid, sid = map(int, STYL_RE.search(p.name).groups())\n",
    "    rows.append((str(p), 1, cid, sid))       # label 1 = stylized\n",
    "\n",
    "# ---- 1.4  Write CSV -------------------------------------------------------\n",
    "with OUT.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"path\", \"label\", \"content_id\", \"style_id\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Manifest written → {OUT}, total rows = {len(rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MANIFEST = \"images\\\\manifest.csv\"\n",
    "SEED     = 42\n",
    "\n",
    "df = pd.read_csv(MANIFEST)\n",
    "\n",
    "# ---- 2.1  Unique IDs ------------------------------------------------------\n",
    "content_ids = df.loc[df.content_id != -1, \"content_id\"].unique()\n",
    "style_ids   = df.loc[df.style_id   != -1, \"style_id\"  ].unique()\n",
    "\n",
    "# ---- 2.2  20 % hold-out ---------------------------------------------------\n",
    "c_train, c_test = train_test_split(content_ids, test_size=0.2, random_state=SEED)\n",
    "s_train, s_test = train_test_split(style_ids,   test_size=0.2, random_state=SEED)\n",
    "\n",
    "# ---- 2.3  Boolean masks ---------------------------------------------------\n",
    "def in_train(row):\n",
    "    c_ok = (row.content_id == -1) or (row.content_id in c_train)\n",
    "    s_ok = (row.style_id   == -1) or (row.style_id   in s_train)\n",
    "    return c_ok and s_ok           # both IDs must be in *train* lists\n",
    "\n",
    "df[\"split\"] = np.where(df.apply(in_train, axis=1), \"train\", \"test\")\n",
    "\n",
    "# ---- 2.4  Save ------------------------------------------------------------\n",
    "df.to_csv(MANIFEST, index=False)\n",
    "print(df[\"split\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e1d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "from dataset import StyleDetectDataset, train_tf, val_tf\n",
    "\n",
    "CSV = \"images\\\\manifest.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- 4.1  Datasets & loaders ---------------------------------------------\n",
    "train_ds = StyleDetectDataset(CSV, split=\"train\", transforms=train_tf)\n",
    "val_ds   = StyleDetectDataset(CSV, split=\"test\",  transforms=val_tf)\n",
    "\n",
    "# Imbalance handling: weight inversely to class freq\n",
    "labels = [y for _,y in train_ds]\n",
    "class_count = torch.bincount(torch.tensor(labels))\n",
    "class_weight = 1.0 / class_count.float()\n",
    "weights = class_weight[labels]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_ds), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, sampler=sampler,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "# ---- 4.2  Model -----------------------------------------------------------\n",
    "model = torchvision.models.resnet50(weights=\"IMAGENET1K_V2\")  # pre-trained\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)                 # 2-class head\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ---- 4.3  Optimiser, loss, sched -----------------------------------------\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weight.to(DEVICE))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# ---- 4.4  Train loop ------------------------------------------------------\n",
    "EPOCHS = 3\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train(); running_loss = 0.0\n",
    "    for x,y in tqdm(train_loader, desc=f\"Train {epoch}\"):\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---- validation ----\n",
    "    model.eval(); correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_loader:\n",
    "            x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x).argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total   += y.size(0)\n",
    "    acc = correct/total*100\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={running_loss/len(train_ds):.4f} \"\n",
    "          f\"| val_acc={acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e52699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the final epoch finishes\n",
    "torch.save(model, \"resnet_style.pt\")   # saves the entire model object\n",
    "print(\"✔ model saved → resnet_style.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16e00f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Predictions: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision.transforms as T\n",
    "import torch.serialization\n",
    "import torchvision.models.resnet\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import Label\n",
    "\n",
    "\n",
    "# ---- 2.1  Pick a device ---------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---- 2.2  Load the saved model on that device -----------------------------\n",
    "with torch.serialization.safe_globals([torchvision.models.resnet.ResNet]):\n",
    "    model = torch.load(\"resnet_style.pt\", map_location=device, weights_only=False)\n",
    "model.eval()                       # set to inference mode\n",
    "model.to(device)                   # make sure weights live on the right GPU/CPU\n",
    "\n",
    "# ---- 2.3  Re-use the same val transform ----------------------------------\n",
    "tf = T.Compose([\n",
    "    T.Resize(256), T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406],\n",
    "                [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ---- 2.4  Load two images *with safe paths* -------------------------------\n",
    "imgA = Image.open(r\"images\\\\contents\\\\content_1.jpg\")\n",
    "imgB = Image.open(r\"images\\stylizations\\content_1___style_16___700.jpg\")\n",
    "\n",
    "# ---- 2.5  Apply transformations (only for model input) -------------------\n",
    "imgA_tensor = tf(imgA.convert(\"RGB\")).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "imgB_tensor = tf(imgB.convert(\"RGB\")).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "batch = torch.cat([imgA_tensor, imgB_tensor], dim=0).to(device)\n",
    "\n",
    "# ---- 2.6  Predict ---------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    preds = model(batch).argmax(1).cpu().tolist()  # 0 = real, 1 = stylized\n",
    "\n",
    "print(\"Predictions:\", preds)  # e.g. [1, 0]\n",
    "\n",
    "# ---- 2.7  Create the UI to display images side by side -------------------\n",
    "\n",
    "# Set up the Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Image Comparison\")\n",
    "\n",
    "# Resize images before converting to Tkinter-compatible format (only for display)\n",
    "imgA_resized = imgA.resize((224, 224))  # Resize image to fit in window\n",
    "imgB_resized = imgB.resize((224, 224))  # Resize image to fit in window\n",
    "\n",
    "# Convert the resized images to Tkinter-compatible format\n",
    "imgA_tk = ImageTk.PhotoImage(imgA_resized)\n",
    "imgB_tk = ImageTk.PhotoImage(imgB_resized)\n",
    "\n",
    "# Store image references globally to prevent garbage collection\n",
    "global imgA_tk_ref, imgB_tk_ref\n",
    "imgA_tk_ref = imgA_tk  # Store the reference to image A\n",
    "imgB_tk_ref = imgB_tk  # Store the reference to image B\n",
    "\n",
    "# Create labels to display the images\n",
    "labelA = Label(root, image=imgA_tk)\n",
    "labelA.grid(row=0, column=0, padx=10, pady=10)\n",
    "\n",
    "labelB = Label(root, image=imgB_tk)\n",
    "labelB.grid(row=0, column=1, padx=10, pady=10)\n",
    "\n",
    "# Add labels for predictions\n",
    "pred_labelA = Label(root, text=f\"Prediction: {preds[0]}\")\n",
    "pred_labelA.grid(row=1, column=0)\n",
    "\n",
    "pred_labelB = Label(root, text=f\"Prediction: {preds[1]}\")\n",
    "pred_labelB.grid(row=1, column=1)\n",
    "\n",
    "# Run the Tkinter main loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"images/manifest.csv\")\n",
    "print(df[df.split==\"test\"].label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1321b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "from dataset import StyleDetectDataset, train_tf, val_tf\n",
    "\n",
    "CSV = \"images\\\\manifest.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- 5.1  Data loaders (same as ResNet script) ----------------------------\n",
    "train_ds = StyleDetectDataset(CSV, split=\"train\", transforms=train_tf)\n",
    "val_ds   = StyleDetectDataset(CSV, split=\"test\",  transforms=val_tf)\n",
    "\n",
    "labels = [y for _,y in train_ds]\n",
    "class_count = torch.bincount(torch.tensor(labels))\n",
    "class_weight = 1.0 / class_count.float()\n",
    "weights = class_weight[labels]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_ds), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, sampler=sampler,  # ViT uses smaller batch\n",
    "                          num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "# ---- 5.2  ViT backbone ----------------------------------------------------\n",
    "model = torchvision.models.vit_b_16(weights=\"IMAGENET1K_V1\")  # ViT-Base\n",
    "model.heads.head = nn.Linear(model.heads.head.in_features, 2) # 2-class\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ---- 5.3  Optim + sched ---------------------------------------------------\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weight.to(DEVICE))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)   # higher LR for ViT\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# ---- 5.4  Training loop ---------------------------------------------------\n",
    "EPOCHS = 3\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train(); running=0\n",
    "    for x,y in tqdm(train_loader, desc=f\"Train {epoch}\"):\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item()*x.size(0)\n",
    "    scheduler.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval(); correct=total=0\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_loader:\n",
    "            x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x).argmax(1)\n",
    "            correct += (preds==y).sum().item()\n",
    "            total   += y.size(0)\n",
    "    print(f\"Epoch {epoch:02d} | loss={running/len(train_ds):.4f} \"\n",
    "          f\"| val_acc={correct/total*100:.2f}%\")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "all_preds, all_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        logits = model(x.to(DEVICE))\n",
    "        probs  = torch.softmax(logits, 1)[:, 1]           # P(class=1)\n",
    "        all_preds.extend(probs.cpu().numpy())\n",
    "        all_true.extend(y.numpy())\n",
    "\n",
    "torch.save(model, \"vit_model.pt\")\n",
    "print(\"✔ model saved to vit_model.pt\")\n",
    "\n",
    "print(confusion_matrix(all_true, np.array(all_preds) > 0.5))\n",
    "print(classification_report(all_true, np.array(all_preds) > 0.5, digits=3))\n",
    "print(\"AUC =\", roc_auc_score(all_true, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddbb5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Evaluate a saved ViT (or any binary model) and pick the best threshold\n",
    "according to F1-score on the *test* split.\n",
    "-----------------------------------------------------------------------\n",
    "Usage:\n",
    "    python eval_with_threshold.py vit_epoch02.pt\n",
    "\"\"\"\n",
    "\n",
    "import sys, torch, numpy as np\n",
    "import torchvision.transforms as T\n",
    "import torch.serialization\n",
    "import torchvision.models.resnet\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from dataset import StyleDetectDataset, val_tf   # same transform as training\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) CONFIG\n",
    "MODEL_PATH = \"vit_model_epoch02.pt\"            # e.g. \"vit_epoch02.pt\"\n",
    "CSV        = r\"images/manifest.csv\"\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64                         # bigger is fine for inference\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) DATA LOADER  (test split only)\n",
    "test_ds     = StyleDetectDataset(CSV, split=\"test\", transforms=val_tf)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Test samples: {len(test_ds)} (label distribution: \"\n",
    "      f\"{np.bincount([y for _,y in test_ds])})\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) LOAD MODEL\n",
    "with torch.serialization.safe_globals([torchvision.models.resnet.ResNet]):\n",
    "    model = torch.load(MODEL_PATH, map_location=DEVICE, weights_only = False)\n",
    "model.eval().to(DEVICE)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) COLLECT PROBABILITIES & LABELS\n",
    "all_probs, all_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        logits = model(x.to(DEVICE))\n",
    "        probs  = torch.softmax(logits, dim=1)[:, 1]        # P(class==1)\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_true.extend(y.cpu().numpy())\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "all_true  = np.array(all_true)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) FIND THRESHOLD THAT MAXIMISES F1\n",
    "prec, rec, thresh = precision_recall_curve(all_true, all_probs)\n",
    "f1 = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "best_idx   = np.argmax(f1)\n",
    "best_thr   = thresh[best_idx]\n",
    "best_f1    = f1[best_idx]\n",
    "\n",
    "print(f\"\\n>>>  Best F1 = {best_f1:.3f} at threshold {best_thr:.3f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) FINAL METRICS WITH THE NEW THRESHOLD\n",
    "final_pred = (all_probs >= best_thr).astype(int)\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(all_true, final_pred))\n",
    "\n",
    "print(\"\\nClassification report (macro-averaged):\")\n",
    "print(classification_report(all_true, final_pred, digits=3))\n",
    "\n",
    "print(f\"AUC  = {roc_auc_score(all_true, all_probs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch, torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "MODEL_PATH = sys.argv[1]    # path to .pt or .pth weights (ResNet or ViT)\n",
    "IMG_A      = sys.argv[2]    # first image path\n",
    "IMG_B      = sys.argv[3]    # second image path\n",
    "\n",
    "# ---- 6.1  Load model (auto-detect backbone type) --------------------------\n",
    "model = torch.load(MODEL_PATH)\n",
    "model.eval().cuda()\n",
    "\n",
    "# ---- 6.2  Same val transforms as training --------------------------------\n",
    "tf = T.Compose([\n",
    "    T.Resize(256), T.CenterCrop(224),\n",
    "    T.ToTensor(), T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# ---- 6.3  Prepare batch of two -------------------------------------------\n",
    "batch = torch.stack([tf(Image.open(p).convert(\"RGB\")) for p in (IMG_A, IMG_B)])\n",
    "batch = batch.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(batch).argmax(1).cpu().tolist()  # 0=real,1=fake\n",
    "\n",
    "print(f\"Prediction: {preds[0]} {preds[1]}  (0 = real, 1 = style-transfer)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
