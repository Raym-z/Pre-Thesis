{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f25b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio\n",
    "# %pip install pandas scikit-learn\n",
    "# %pip install pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f7d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images: {0: 5000, 1: 5000}\n",
      "After balancing: {1: 5000, 0: 5000}\n",
      "Train/test split: {'train': 8000, 'test': 2000}\n",
      "Final class counts: {0: 5000, 1: 5000}\n",
      "Wrote 10000 rows to images\\manifest10k.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---- 1. Config -------------------------------------------------------------\n",
    "ROOT = pathlib.Path(\"images\")\n",
    "OUT  = ROOT / \"manifest10k.csv\"\n",
    "SEED = 42\n",
    "\n",
    "# ---- 2. Gather all image paths & labels -----------------------------------\n",
    "rows = []\n",
    "for folder, label in [\n",
    "    (\"contents\",     0),   # real originals\n",
    "    (\"styles\",       0),   # real style-only\n",
    "    (\"stylizations\", 1),   # stylized (fake)\n",
    "]:\n",
    "    for p in (ROOT / folder).glob(\"*.jpg\"):\n",
    "        rows.append((str(p), label))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"path\",\"label\"])\n",
    "print(f\"Found {len(df)} images: {df.label.value_counts().to_dict()}\")\n",
    "\n",
    "# ---- 3. Down-sample to at most 5k per class ------------------------------\n",
    "max_per_label = 5000\n",
    "balanced_parts = []\n",
    "for lbl in (0, 1):\n",
    "    part = df[df.label == lbl]\n",
    "    n    = min(len(part), max_per_label)\n",
    "    balanced_parts.append(part.sample(n=n, random_state=SEED))\n",
    "\n",
    "balanced = pd.concat(balanced_parts).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "print(f\"After balancing: {balanced.label.value_counts().to_dict()}\")\n",
    "\n",
    "# ---- 4. Stratified train/test split (80/20) -------------------------------\n",
    "train_df, test_df = train_test_split(\n",
    "    balanced,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=balanced[\"label\"]\n",
    ")\n",
    "\n",
    "train_df[\"split\"] = \"train\"\n",
    "test_df [\"split\"] = \"test\"\n",
    "\n",
    "result = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "print(f\"Train/test split: {result.split.value_counts().to_dict()}\")\n",
    "print(f\"Final class counts: {result.label.value_counts().to_dict()}\")\n",
    "\n",
    "# ---- 5. Save manifest.csv -------------------------------------------------\n",
    "result.to_csv(OUT, index=False)\n",
    "print(f\"Wrote {len(result)} rows to {OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a988cec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train    8000\n",
      "test     2000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---- 1. Config -------------------------------------------------------------\n",
    "MANIFEST = \"images\\\\manifest10k.csv\"  # or use forward‐slashes: \"images/manifest10k.csv\"\n",
    "SEED     = 42\n",
    "\n",
    "# ---- 2. Load ----------------------------------------------------------------\n",
    "df = pd.read_csv(MANIFEST)\n",
    "\n",
    "# ---- 3. Stratified train/test split (80/20) ---------------------------------\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_df[\"split\"] = \"train\"\n",
    "test_df [\"split\"] = \"test\"\n",
    "\n",
    "# ---- 4. Save ---------------------------------------------------------------\n",
    "out = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "out.to_csv(MANIFEST, index=False)\n",
    "\n",
    "# ---- 5. Summary ------------------------------------------------------------\n",
    "print(out[\"split\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e1d95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1: 100%|██████████| 125/125 [10:18<00:00,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.4723 | val_acc=80.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2: 100%|██████████| 125/125 [10:22<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train_loss=0.3217 | val_acc=80.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3: 100%|██████████| 125/125 [10:10<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train_loss=0.2396 | val_acc=76.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4: 100%|██████████| 125/125 [10:02<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train_loss=0.1899 | val_acc=80.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5: 100%|██████████| 125/125 [10:19<00:00,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train_loss=0.1398 | val_acc=78.75%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import StyleDetectDataset, train_tf, val_tf\n",
    "\n",
    "CSV = \"images\\\\manifest10k.csv\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- 4.1  Datasets & loaders ---------------------------------------------\n",
    "train_ds = StyleDetectDataset(str(CSV), split=\"train\", transforms=train_tf)\n",
    "val_ds   = StyleDetectDataset(str(CSV), split=\"test\",  transforms=val_tf)\n",
    "\n",
    "# Imbalance handling: weight inversely to class freq\n",
    "labels = [y for _, y in train_ds]\n",
    "class_count = torch.bincount(torch.tensor(labels))\n",
    "class_weight = 1.0 / class_count.float()\n",
    "weights = class_weight[labels]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_ds), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, sampler=sampler,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "# ---- 4.2  Model -----------------------------------------------------------\n",
    "model = torchvision.models.resnet50(weights=\"IMAGENET1K_V2\")  # pre-trained\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)                 # 2-class head\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ---- 4.3  Optimiser, loss, sched -----------------------------------------\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weight.to(DEVICE))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# ---- 4.4  Train loop ------------------------------------------------------\n",
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train(); running_loss = 0.0\n",
    "    for x, y in tqdm(train_loader, desc=f\"Train {epoch}\"):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---- validation ----\n",
    "    model.eval(); correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x).argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total   += y.size(0)\n",
    "    acc = correct/total*100\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={running_loss/len(train_ds):.4f} \"\n",
    "          f\"| val_acc={acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e52699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ model saved → resnet_style.pt\n"
     ]
    }
   ],
   "source": [
    "# after the final epoch finishes\n",
    "torch.save(model, \"resnet_style.pt\")   # saves the entire model object\n",
    "print(\"✔ model saved → resnet_style.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e00f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch, torchvision.transforms as T\n",
    "# import torch.serialization\n",
    "# import torchvision.models.resnet\n",
    "# from PIL import Image, ImageTk\n",
    "# import tkinter as tk\n",
    "# from tkinter import Label\n",
    "\n",
    "\n",
    "# # ---- 2.1  Pick a device ---------------------------------------------------\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Using device:\", device)\n",
    "\n",
    "# # ---- 2.2  Load the saved model on that device -----------------------------\n",
    "# with torch.serialization.safe_globals([torchvision.models.resnet.ResNet]):\n",
    "#     model = torch.load(\"resnet_style.pt\", map_location=device, weights_only=False)\n",
    "# model.eval()                       # set to inference mode\n",
    "# model.to(device)                   # make sure weights live on the right GPU/CPU\n",
    "\n",
    "# # ---- 2.3  Re-use the same val transform ----------------------------------\n",
    "# tf = T.Compose([\n",
    "#     T.Resize(256), T.CenterCrop(224),\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize([0.485, 0.456, 0.406],\n",
    "#                 [0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# # ---- 2.4  Load two images *with safe paths* -------------------------------\n",
    "# imgA = Image.open(r\"images\\\\contents\\\\content_1.jpg\")\n",
    "# imgB = Image.open(r\"images\\\\stylizations\\\\content_1___style_16___700.jpg\")\n",
    "\n",
    "# # ---- 2.5  Apply transformations (only for model input) -------------------\n",
    "# imgA_tensor = tf(imgA.convert(\"RGB\")).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "# imgB_tensor = tf(imgB.convert(\"RGB\")).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "# batch = torch.cat([imgA_tensor, imgB_tensor], dim=0).to(device)\n",
    "\n",
    "# # ---- 2.6  Predict ---------------------------------------------------------\n",
    "# with torch.no_grad():\n",
    "#     preds = model(batch).argmax(1).cpu().tolist()  # 0 = real, 1 = stylized\n",
    "\n",
    "# print(\"Predictions:\", preds)  # e.g. [1, 0]\n",
    "\n",
    "# # ---- 2.7  Create the UI to display images side by side -------------------\n",
    "\n",
    "# # Set up the Tkinter window\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Image Comparison\")\n",
    "\n",
    "# # Resize images before converting to Tkinter-compatible format (only for display)\n",
    "# imgA_resized = imgA.resize((224, 224))  # Resize image to fit in window\n",
    "# imgB_resized = imgB.resize((224, 224))  # Resize image to fit in window\n",
    "\n",
    "# # Convert the resized images to Tkinter-compatible format\n",
    "# imgA_tk = ImageTk.PhotoImage(imgA_resized)\n",
    "# imgB_tk = ImageTk.PhotoImage(imgB_resized)\n",
    "\n",
    "# # Store image references globally to prevent garbage collection\n",
    "# global imgA_tk_ref, imgB_tk_ref\n",
    "# imgA_tk_ref = imgA_tk  # Store the reference to image A\n",
    "# imgB_tk_ref = imgB_tk  # Store the reference to image B\n",
    "\n",
    "# # Create labels to display the images\n",
    "# labelA = Label(root, image=imgA_tk)\n",
    "# labelA.grid(row=0, column=0, padx=10, pady=10)\n",
    "\n",
    "# labelB = Label(root, image=imgB_tk)\n",
    "# labelB.grid(row=0, column=1, padx=10, pady=10)\n",
    "\n",
    "# # Add labels for predictions\n",
    "# pred_labelA = Label(root, text=f\"Prediction: {preds[0]}\")\n",
    "# pred_labelA.grid(row=1, column=0)\n",
    "\n",
    "# pred_labelB = Label(root, text=f\"Prediction: {preds[1]}\")\n",
    "# pred_labelB.grid(row=1, column=1)\n",
    "\n",
    "# # Run the Tkinter main loop\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23838c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RESNET COMPREHENSIVE EVALUATION\n",
      "============================================================\n",
      "\n",
      "1. CONFUSION MATRIX:\n",
      "   Predicted\n",
      "Actual  0    1\n",
      "  0    804  196\n",
      "  1    229  771\n",
      "\n",
      "2. CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real      0.778     0.804     0.791      1000\n",
      "    Stylized      0.797     0.771     0.784      1000\n",
      "\n",
      "    accuracy                          0.787      2000\n",
      "   macro avg      0.788     0.788     0.787      2000\n",
      "weighted avg      0.788     0.787     0.787      2000\n",
      "\n",
      "\n",
      "3. ROC-AUC SCORE: 0.876\n",
      "\n",
      "4. ADDITIONAL METRICS:\n",
      "   Precision: 0.797\n",
      "   Recall:    0.771\n",
      "   F1-Score:  0.784\n",
      "   Accuracy:  0.787\n",
      "\n",
      "5. CLASS DISTRIBUTION:\n",
      "   Real images:     1000\n",
      "   Stylized images: 1000\n",
      "\n",
      "============================================================\n",
      "Evaluation complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Comprehensive evaluation for ResNet model\n",
    "Add this cell after your ResNet training is complete\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from dataset import StyleDetectDataset, train_tf, val_tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "CSV = \"images\\\\manifest10k.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "val_ds   = StyleDetectDataset(CSV, split=\"test\",  transforms=val_tf)\n",
    "\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESNET COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect all predictions and true labels\n",
    "all_preds, all_true, all_probs = [], [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]  # Probability of class 1 (stylized)\n",
    "        preds = logits.argmax(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_true.extend(y.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_true = np.array(all_true)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "# Calculate and display metrics\n",
    "print(\"\\n1. CONFUSION MATRIX:\")\n",
    "print(\"   Predicted\")\n",
    "print(\"Actual  0    1\")\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "print(\"  0    {}  {}\".format(cm[0, 0], cm[0, 1]))\n",
    "print(\"  1    {}  {}\".format(cm[1, 0], cm[1, 1]))\n",
    "\n",
    "print(\"\\n2. CLASSIFICATION REPORT:\")\n",
    "print(classification_report(all_true, all_preds, digits=3, target_names=['Real', 'Stylized']))\n",
    "\n",
    "print(f\"\\n3. ROC-AUC SCORE: {roc_auc_score(all_true, all_probs):.3f}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(all_true, all_preds)\n",
    "recall = recall_score(all_true, all_preds)\n",
    "f1 = f1_score(all_true, all_preds)\n",
    "accuracy = (all_preds == all_true).mean()\n",
    "\n",
    "print(f\"\\n4. ADDITIONAL METRICS:\")\n",
    "print(f\"   Precision: {precision:.3f}\")\n",
    "print(f\"   Recall:    {recall:.3f}\")\n",
    "print(f\"   F1-Score:  {f1:.3f}\")\n",
    "print(f\"   Accuracy:  {accuracy:.3f}\")\n",
    "\n",
    "# Class distribution\n",
    "print(f\"\\n5. CLASS DISTRIBUTION:\")\n",
    "print(f\"   Real images:     {np.sum(all_true == 0)}\")\n",
    "print(f\"   Stylized images: {np.sum(all_true == 1)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluation complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd1321b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: privateuseone:0\n",
      "DirectML detected — training in fp32 (no AMP).\n",
      "\n",
      "==> Stage A (head only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0 <= device.index() && device.index() < static_cast<c10::DeviceIndex>(device_ready_queues_.size()) INTERNAL ASSERT FAILED at \"C:\\\\actions-runner\\\\_work\\\\pytorch\\\\pytorch\\\\builder\\\\windows\\\\pytorch\\\\torch\\\\csrc\\\\autograd\\\\engine.cpp\":1451, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 195\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# run epochs\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, stage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 195\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m | Train \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     val_acc, y_true, y_prob \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, DEVICE)\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 120\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, scheduler, device, scaler, desc)\u001b[0m\n\u001b[0;32m    118\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# DirectML path\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    123\u001b[0m running \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0 <= device.index() && device.index() < static_cast<c10::DeviceIndex>(device_ready_queues_.size()) INTERNAL ASSERT FAILED at \"C:\\\\actions-runner\\\\_work\\\\pytorch\\\\pytorch\\\\builder\\\\windows\\\\pytorch\\\\torch\\\\csrc\\\\autograd\\\\engine.cpp\":1451, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# --- YOUR IMPORTS -----------------------------------------------------------\n",
    "import torch, torchvision\n",
    "import torch_directml                              # (kept)\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "from dataset import StyleDetectDataset, train_tf, val_tf\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "import contextlib\n",
    "\n",
    "# --- CONFIG (kept / small tweaks) ------------------------------------------\n",
    "CSV = \"images\\\\manifest10k.csv\"\n",
    "# NEW: robust device pick (CUDA -> DirectML -> CPU)\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    import torch_directml\n",
    "    DEVICE = torch_directml.device(0)  # DirectML GPU 0, avoids index bug\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ---- Data loaders (unchanged) ----------------------------------------------\n",
    "train_ds = StyleDetectDataset(CSV, split=\"train\", transforms=train_tf)\n",
    "val_ds   = StyleDetectDataset(CSV, split=\"test\",  transforms=val_tf)\n",
    "\n",
    "labels = [y for _, y in train_ds]\n",
    "class_count = torch.bincount(torch.tensor(labels))\n",
    "class_weight = 1.0 / class_count.float()\n",
    "weights = class_weight[labels]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_ds), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, sampler=sampler,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "# ---- ViT backbone (mostly unchanged) ---------------------------------------\n",
    "model = torchvision.models.vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "model.heads.head = nn.Linear(model.heads.head.in_features, 2)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ---- Loss (NEW: label smoothing) -------------------------------------------\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "\n",
    "# ---- AMP helpers (NEW: CUDA AMP; best-effort DML; fallback no-AMP) ---------\n",
    "def get_amp_context_and_scaler(device):\n",
    "    if isinstance(device, torch.device) and device.type == \"cuda\":\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "        autocast_ctx = lambda: torch.cuda.amp.autocast(dtype=torch.float16)\n",
    "        return autocast_ctx, scaler\n",
    "\n",
    "    # DirectML path — disable AMP entirely for stability\n",
    "    print(\"DirectML detected — training in fp32 (no AMP).\")\n",
    "    return contextlib.nullcontext, None\n",
    "\n",
    "autocast, scaler = get_amp_context_and_scaler(DEVICE)\n",
    "\n",
    "# ---- Param groups & stage helpers (NEW) ------------------------------------\n",
    "def make_param_groups(model, lr_head, lr_backbone, wd=0.05):\n",
    "    groups = [\n",
    "        {\"params\": list(model.heads.parameters()), \"lr\": lr_head, \"weight_decay\": wd},\n",
    "        {\"params\": [], \"lr\": lr_backbone, \"weight_decay\": wd},\n",
    "    ]\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad and not n.startswith(\"heads\"):\n",
    "            groups[1][\"params\"].append(p)\n",
    "    return groups\n",
    "\n",
    "def freeze_all(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_head(model):\n",
    "    for p in model.heads.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def unfreeze_last_blocks(model, k=4):\n",
    "    for p in model.encoder.layers[-k:].parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def unfreeze_all(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "# ---- Scheduler builder (NEW: warmup -> cosine, step-per-batch) -------------\n",
    "def build_scheduler(optimizer, steps_total, warmup_ratio=0.05):\n",
    "    warmup_steps = max(1, int(steps_total * warmup_ratio))\n",
    "    cosine_steps = max(1, steps_total - warmup_steps)\n",
    "    warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=0.01, total_iters=warmup_steps\n",
    "    )\n",
    "    cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=cosine_steps\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer, schedulers=[warmup, cosine], milestones=[warmup_steps]\n",
    "    )\n",
    "    return scheduler\n",
    "\n",
    "# ---- Train/eval loops (NEW: AMP + per-step scheduler) ----------------------\n",
    "def train_one_epoch(model, loader, optimizer, scheduler, device, scaler, desc):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    pbar = tqdm(loader, desc=desc, leave=False)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast():  # will be nullcontext on DirectML\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        if scaler is not None:  # CUDA path\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:  # DirectML path\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running += loss.item() * x.size(0)\n",
    "        scheduler.step()\n",
    "    return running / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    all_probs, all_true = [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total   += y.size(0)\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_true.extend(y.cpu().numpy())\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    return acc, np.array(all_true), np.array(all_probs)\n",
    "\n",
    "# ---- STAGED FINE-TUNING (NEW) ----------------------------------------------\n",
    "# stage_cfgs = [\n",
    "#     {\"name\": \"Stage A (head only)\",\n",
    "#      \"setup\": lambda m: (freeze_all(m), unfreeze_head(m)),\n",
    "#      \"lr_head\": 1e-3, \"lr_backbone\": 0.0, \"epochs\": 3},\n",
    "\n",
    "#     {\"name\": \"Stage B (last 4 blocks + head)\",\n",
    "#      \"setup\": lambda m: (freeze_all(m), unfreeze_head(m), unfreeze_last_blocks(m, k=4)),\n",
    "#      \"lr_head\": 1e-3, \"lr_backbone\": 1e-4, \"epochs\": 5},\n",
    "\n",
    "#     {\"name\": \"Stage C (full model)\",\n",
    "#      \"setup\": lambda m: unfreeze_all(m),\n",
    "#      \"lr_head\": 3e-5, \"lr_backbone\": 3e-5, \"epochs\": 8},\n",
    "# ]\n",
    "\n",
    "# ---- FAST RUN CONFIG (shorter epochs) ----\n",
    "stage_cfgs = [\n",
    "    {\"name\": \"Stage A (head only)\",\n",
    "     \"setup\": lambda m: (freeze_all(m), unfreeze_head(m)),\n",
    "     \"lr_head\": 1e-3, \"lr_backbone\": 0.0, \"epochs\": 1},  # was 3\n",
    "\n",
    "    {\"name\": \"Stage B (last 4 blocks + head)\",\n",
    "     \"setup\": lambda m: (freeze_all(m), unfreeze_head(m), unfreeze_last_blocks(m, k=4)),\n",
    "     \"lr_head\": 1e-3, \"lr_backbone\": 1e-4, \"epochs\": 2},  # was 5\n",
    "\n",
    "    {\"name\": \"Stage C (full model)\",\n",
    "     \"setup\": lambda m: unfreeze_all(m),\n",
    "     \"lr_head\": 3e-5, \"lr_backbone\": 3e-5, \"epochs\": 3},  # was 8\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "best_val = 0.0\n",
    "for stage in stage_cfgs:\n",
    "    print(\"\\n==>\", stage[\"name\"])\n",
    "    stage[\"setup\"](model)\n",
    "\n",
    "    # build optimizer with discriminative LRs\n",
    "    # (backbone group will be empty in Stage A because requires_grad=False)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        make_param_groups(model, lr_head=stage[\"lr_head\"], lr_backbone=stage[\"lr_backbone\"], wd=0.05)\n",
    "    )\n",
    "\n",
    "    # per-step scheduler\n",
    "    steps_per_epoch = max(1, len(train_loader))\n",
    "    total_steps = steps_per_epoch * stage[\"epochs\"]\n",
    "    scheduler = build_scheduler(optimizer, total_steps, warmup_ratio=0.05)\n",
    "\n",
    "    # run epochs\n",
    "    for epoch in range(1, stage[\"epochs\"] + 1):\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, scheduler, DEVICE, scaler,\n",
    "            desc=f\"{stage['name']} | Train {epoch}/{stage['epochs']}\"\n",
    "        )\n",
    "        val_acc, y_true, y_prob = evaluate(model, val_loader, DEVICE)\n",
    "        print(f\"{stage['name']} | Epoch {epoch:02d} | loss={train_loss:.4f} | val_acc={val_acc*100:.2f}%\")\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            torch.save(model.state_dict(), \"vit_model_best.pt\")\n",
    "            print(\"  ↳ Saved best to vit_model_best.pt\")\n",
    "\n",
    "# ---- Final evaluation & reports (minor tweaks) -----------------------------\n",
    "print(\"\\nEvaluating best checkpoint...\")\n",
    "model.load_state_dict(torch.load(\"vit_model_best.pt\", map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "val_acc, all_true, all_probs = evaluate(model, val_loader, DEVICE)\n",
    "print(f\"Best Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "# threshold at 0.5 (you can tune threshold via ROC later)\n",
    "pred_bin = (all_probs > 0.5).astype(np.int64)\n",
    "print(confusion_matrix(all_true, pred_bin))\n",
    "print(classification_report(all_true, pred_bin, digits=3))\n",
    "print(\"AUC =\", roc_auc_score(all_true, all_probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ddbb5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 2000 (label distribution: [1000 1000])\n",
      "\n",
      ">>>  Best F1 = 0.776 at threshold 0.459\n",
      "\n",
      "Confusion matrix:\n",
      "[[611 389]\n",
      " [120 880]]\n",
      "\n",
      "Classification report (macro-averaged):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.836     0.611     0.706      1000\n",
      "           1      0.693     0.880     0.776      1000\n",
      "\n",
      "    accuracy                          0.746      2000\n",
      "   macro avg      0.765     0.746     0.741      2000\n",
      "weighted avg      0.765     0.746     0.741      2000\n",
      "\n",
      "AUC  = 0.806\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Evaluate a saved ViT (or any binary model) and pick the best threshold\n",
    "according to F1-score on the *test* split.\n",
    "-----------------------------------------------------------------------\n",
    "Usage:\n",
    "    python eval_with_threshold.py vit_epoch02.pt\n",
    "\"\"\"\n",
    "\n",
    "import sys, torch, numpy as np\n",
    "import torchvision.transforms as T\n",
    "import torch.serialization\n",
    "import torchvision.models.resnet\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from dataset import StyleDetectDataset, val_tf   # same transform as training\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) CONFIG\n",
    "MODEL_PATH = r\"vit_model.pt\"            # e.g. \"vit_epoch02.pt\"\n",
    "CSV        = r\"images\\\\manifest10k.csv\"\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64                         # bigger is fine for inference\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) DATA LOADER  (test split only)\n",
    "test_ds     = StyleDetectDataset(CSV, split=\"test\", transforms=val_tf)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Test samples: {len(test_ds)} (label distribution: \"\n",
    "      f\"{np.bincount([y for _,y in test_ds])})\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) LOAD MODEL\n",
    "model = torch.load(MODEL_PATH, map_location=DEVICE, weights_only = False)\n",
    "model.eval().to(DEVICE)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) COLLECT PROBABILITIES & LABELS\n",
    "all_probs, all_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        logits = model(x.to(DEVICE))\n",
    "        probs  = torch.softmax(logits, dim=1)[:, 1]        # P(class==1)\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_true.extend(y.cpu().numpy())\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "all_true  = np.array(all_true)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) FIND THRESHOLD THAT MAXIMISES F1\n",
    "prec, rec, thresh = precision_recall_curve(all_true, all_probs)\n",
    "f1 = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "best_idx   = np.argmax(f1)\n",
    "best_thr   = thresh[best_idx]\n",
    "best_f1    = f1[best_idx]\n",
    "\n",
    "print(f\"\\n>>>  Best F1 = {best_f1:.3f} at threshold {best_thr:.3f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) FINAL METRICS WITH THE NEW THRESHOLD\n",
    "final_pred = (all_probs >= best_thr).astype(int)\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(all_true, final_pred))\n",
    "\n",
    "print(\"\\nClassification report (macro-averaged):\")\n",
    "print(classification_report(all_true, final_pred, digits=3))\n",
    "\n",
    "print(f\"AUC  = {roc_auc_score(all_true, all_probs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b6a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python\n",
    "# import torch, torchvision.transforms as T\n",
    "# from PIL import Image\n",
    "# import sys\n",
    "\n",
    "# MODEL_PATH = sys.argv[1]    # path to .pt or .pth weights (ResNet or ViT)\n",
    "# IMG_A      = sys.argv[2]    # first image path\n",
    "# IMG_B      = sys.argv[3]    # second image path\n",
    "\n",
    "# # ---- 6.1  Load model (auto-detect backbone type) --------------------------\n",
    "# model = torch.load(MODEL_PATH)\n",
    "# model.eval().cuda()\n",
    "\n",
    "# # ---- 6.2  Same val transforms as training --------------------------------\n",
    "# tf = T.Compose([\n",
    "#     T.Resize(256), T.CenterCrop(224),\n",
    "#     T.ToTensor(), T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "# ])\n",
    "\n",
    "# # ---- 6.3  Prepare batch of two -------------------------------------------\n",
    "# batch = torch.stack([tf(Image.open(p).convert(\"RGB\")) for p in (IMG_A, IMG_B)])\n",
    "# batch = batch.cuda()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     preds = model(batch).argmax(1).cpu().tolist()  # 0=real,1=fake\n",
    "\n",
    "# print(f\"Prediction: {preds[0]} {preds[1]}  (0 = real, 1 = style-transfer)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
